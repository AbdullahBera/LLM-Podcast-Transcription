{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting sam_walton.mp3...\n",
      "Exported sam_walton_part0.mp3\n",
      "Exported sam_walton_part1.mp3\n",
      "Exported sam_walton_part2.mp3\n",
      "Exported sam_walton_part3.mp3\n",
      "Splitting li_lu.mp3...\n",
      "Exported li_lu_part0.mp3\n",
      "Exported li_lu_part1.mp3\n",
      "Splitting charlie_munger.mp3...\n",
      "Exported charlie_munger_part0.mp3\n",
      "Exported charlie_munger_part1.mp3\n",
      "Exported charlie_munger_part2.mp3\n",
      "Exported charlie_munger_part3.mp3\n",
      "Exported charlie_munger_part4.mp3\n",
      "Exported charlie_munger_part5.mp3\n",
      "Audio splitting completed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "# Path to the directory where your audio files are stored\n",
    "input_folder = \"/Users/bera/Desktop/audio\"\n",
    "output_folder = \"/Users/bera/Desktop/audio/split_audio\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Duration for each split (in milliseconds, 60000 ms = 1 minute)\n",
    "split_duration = 1200000  \n",
    "\n",
    "# Function to split the audio file into chunks\n",
    "def split_audio(input_file, output_folder, split_duration):\n",
    "    audio = AudioSegment.from_file(input_file)\n",
    "    audio_length = len(audio)\n",
    "    \n",
    "    for i in range(0, audio_length, split_duration):\n",
    "        chunk = audio[i:i + split_duration]\n",
    "        chunk_name = f\"{os.path.splitext(os.path.basename(input_file))[0]}_part{i//split_duration}.mp3\"\n",
    "        chunk.export(os.path.join(output_folder, chunk_name), format=\"mp3\")\n",
    "        print(f\"Exported {chunk_name}\")\n",
    "\n",
    "# Process all audio files in the folder\n",
    "for file_name in os.listdir(input_folder):\n",
    "    if file_name.endswith(\".mp3\") or file_name.endswith(\".wav\"):\n",
    "        input_file_path = os.path.join(input_folder, file_name)\n",
    "        print(f\"Splitting {file_name}...\")\n",
    "        split_audio(input_file_path, output_folder, split_duration)\n",
    "\n",
    "print(\"Audio splitting completed!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 139M/139M [00:03<00:00, 41.9MiB/s]\n",
      "/Users/bera/Desktop/projects/LLM-Podcast-Transcription/whisper_env/lib/python3.11/site-packages/whisper/__init__.py:146: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing /Users/bera/Desktop/audio/split_audio/sam_walton_part2.mp3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bera/Desktop/projects/LLM-Podcast-Transcription/whisper_env/lib/python3.11/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing /Users/bera/Desktop/audio/split_audio/sam_walton_part3.mp3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bera/Desktop/projects/LLM-Podcast-Transcription/whisper_env/lib/python3.11/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing /Users/bera/Desktop/audio/split_audio/sam_walton_part1.mp3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bera/Desktop/projects/LLM-Podcast-Transcription/whisper_env/lib/python3.11/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing /Users/bera/Desktop/audio/split_audio/sam_walton_part0.mp3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bera/Desktop/projects/LLM-Podcast-Transcription/whisper_env/lib/python3.11/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing /Users/bera/Desktop/audio/split_audio/charlie_munger_part5.mp3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bera/Desktop/projects/LLM-Podcast-Transcription/whisper_env/lib/python3.11/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing /Users/bera/Desktop/audio/split_audio/charlie_munger_part4.mp3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bera/Desktop/projects/LLM-Podcast-Transcription/whisper_env/lib/python3.11/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing /Users/bera/Desktop/audio/split_audio/li_lu_part0.mp3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bera/Desktop/projects/LLM-Podcast-Transcription/whisper_env/lib/python3.11/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing /Users/bera/Desktop/audio/split_audio/charlie_munger_part0.mp3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bera/Desktop/projects/LLM-Podcast-Transcription/whisper_env/lib/python3.11/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing /Users/bera/Desktop/audio/split_audio/charlie_munger_part1.mp3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bera/Desktop/projects/LLM-Podcast-Transcription/whisper_env/lib/python3.11/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing /Users/bera/Desktop/audio/split_audio/li_lu_part1.mp3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bera/Desktop/projects/LLM-Podcast-Transcription/whisper_env/lib/python3.11/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing /Users/bera/Desktop/audio/split_audio/charlie_munger_part3.mp3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bera/Desktop/projects/LLM-Podcast-Transcription/whisper_env/lib/python3.11/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing /Users/bera/Desktop/audio/split_audio/charlie_munger_part2.mp3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bera/Desktop/projects/LLM-Podcast-Transcription/whisper_env/lib/python3.11/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcriptions saved to /Users/bera/Desktop/audio/transcriptions!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import whisper\n",
    "\n",
    "# Path to the directory containing split audio files\n",
    "audio_folder = \"/Users/bera/Desktop/audio/split_audio\"\n",
    "# Path to the directory where transcriptions will be saved\n",
    "output_folder = \"/Users/bera/Desktop/audio/transcriptions\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Load the Whisper model (choose from: tiny, base, small, medium, large)\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "# Function to transcribe a single audio file\n",
    "def transcribe_audio(file_path):\n",
    "    print(f\"Transcribing {file_path}...\")\n",
    "    result = model.transcribe(file_path)\n",
    "    return result[\"text\"]\n",
    "\n",
    "# Process all audio files in the folder\n",
    "transcriptions = {}\n",
    "for file_name in os.listdir(audio_folder):\n",
    "    if file_name.endswith(\".mp3\") or file_name.endswith(\".wav\"):\n",
    "        audio_path = os.path.join(audio_folder, file_name)\n",
    "        transcription = transcribe_audio(audio_path)\n",
    "        transcriptions[file_name] = transcription\n",
    "        # Save each transcription to a text file in the output folder\n",
    "        transcription_file = os.path.join(output_folder, f\"{file_name}.txt\")\n",
    "        with open(transcription_file, \"w\") as f:\n",
    "            f.write(transcription)\n",
    "\n",
    "print(f\"Transcriptions saved to {output_folder}!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (whisper_env)",
   "language": "python",
   "name": "whisper_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
